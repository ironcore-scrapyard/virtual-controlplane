{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Virtual Control Plane \u00b6 The project contains a collection of helm charts to deploy a virtual/nodeless Kubernetes API server . Background \u00b6 Why do we need such a thing as a nodeless Kubernetes cluster? When developing a custom control plane by extending Kubernetes and using the Operator Pattern we might be not really interested in objects such Pods , Deployments and others. The goal instead is to introduce new CustomResourceDefinitions on which our controllers will operate by leveraging the concepts of Kubernetes API machinery. In order to do it, we will need certain things such as: etcd server (ideally with a backup and restore sidecar 1 ) kube-api-server and kube-controller-manager Since this Kubernetes API server setup does not have any nodes, there is no need in kube-scheduler as we won't deploy any Pods . Those components can be easily deployed on an existing Kubernetes cluster 2 . The figure above illustrates the core components which are deployed on a vanilla Kubernetes runtime cluster. Now your custom controllers can be deployed on the same (or different) runtime cluster as illustrated in the image above. Advantages \u00b6 The main advantage of this approach is, that controllers and the content inside of virtual Kubernetes API server are no longer physically bound to the underlying runtime cluster as your CRDs are not mixed with content of the runtime clusters API server. Since the virtual controlplane in our scenario is just another workload running in a Kubernetes cluster, it can be moved and restored to a different cluster in case disaster strikes and the runtime cluster becomes unavailable. Another advantage is, that the performance of the virtual Kubernetes API server becomes a lot more predictable since nobody besides your controllers 3 are working against it. In our setup we are reusing the great work from the Gardener project. \u21a9 This can also be done on a classical virtual machine based setup. \u21a9 And maybe the kube-controller-manager . \u21a9","title":"Home"},{"location":"#virtual-control-plane","text":"The project contains a collection of helm charts to deploy a virtual/nodeless Kubernetes API server .","title":"Virtual Control Plane"},{"location":"#background","text":"Why do we need such a thing as a nodeless Kubernetes cluster? When developing a custom control plane by extending Kubernetes and using the Operator Pattern we might be not really interested in objects such Pods , Deployments and others. The goal instead is to introduce new CustomResourceDefinitions on which our controllers will operate by leveraging the concepts of Kubernetes API machinery. In order to do it, we will need certain things such as: etcd server (ideally with a backup and restore sidecar 1 ) kube-api-server and kube-controller-manager Since this Kubernetes API server setup does not have any nodes, there is no need in kube-scheduler as we won't deploy any Pods . Those components can be easily deployed on an existing Kubernetes cluster 2 . The figure above illustrates the core components which are deployed on a vanilla Kubernetes runtime cluster. Now your custom controllers can be deployed on the same (or different) runtime cluster as illustrated in the image above.","title":"Background"},{"location":"#advantages","text":"The main advantage of this approach is, that controllers and the content inside of virtual Kubernetes API server are no longer physically bound to the underlying runtime cluster as your CRDs are not mixed with content of the runtime clusters API server. Since the virtual controlplane in our scenario is just another workload running in a Kubernetes cluster, it can be moved and restored to a different cluster in case disaster strikes and the runtime cluster becomes unavailable. Another advantage is, that the performance of the virtual Kubernetes API server becomes a lot more predictable since nobody besides your controllers 3 are working against it. In our setup we are reusing the great work from the Gardener project. \u21a9 This can also be done on a classical virtual machine based setup. \u21a9 And maybe the kube-controller-manager . \u21a9","title":"Advantages"},{"location":"development/contribution/","text":"Contributors Guide \u00b6 Contributing \u00b6 The Gardener on Metal project uses Github to manage reviews of pull requests. If you are looking to make your first contribution, follow Steps to Contribute If you have a trivial fix or improvement, go ahead and create a pull request and address (with @...) a suitable maintainer of this repository (see CODEOWNERS of this repository) in the description of the pull request. If you plan to do something more involved, first discuss your ideas by creating an issue for this repository. This will avoid unnecessary work and surely give you and us a good deal of inspiration. Note Please follow these style guidelines to have your contribution considered by the maintainers: Coding style guidelines Go Code Review Comments , Formatting and style section of Peter Bourgon\u2019s Go: Best Practices for Production Environments . Steps to Contribute \u00b6 Do you want to work on an issue? You are welcome to claim an existing one by commenting on it in GitHub. Note Perform a cursory search to see if the issue has already been taken by someone else. This will prevent misunderstanding and duplication of effort from contributors on the same issue. If you have questions about one of the issues please comment on them and one of the maintainers will clarify it. We kindly ask you to follow the Pull Request Checklist to ensure reviews can happen accordingly. Contributing Code \u00b6 You are welcome to contribute code to the Gardener on Metal project in order to fix a bug or to implement a new feature. The following rules govern code contributions: Contributions must be licensed under the Apache 2.0 License You need to sign the Developer Certificate of Origin . Contributing Documentation \u00b6 You are welcome to contribute documentation to the Gardener on Metal project. The following rules govern documentation contributions: Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Developer Certificate of Origin . Developer Certificate of Origin \u00b6 Due to legal reasons, contributors will be asked to accept a Developer Certificate of Origin ( DCO ) before they submit the first pull request to the Gardener on Metal project, this happens in an automated fashion during the submission process. We use the standard DCO text of the Linux Foundation . Pull Request Checklist \u00b6 Fork and clone the repository to you local machine. git clone git@github.com:YOUR_GITHUB_USER/virtual-controlplane.git cd virtual-controlplane Create a branch from the main using 'git checkout' command. Note If needed, rebase to the current main branch before submitting your pull request. If it doesn't merge properly with main you may be asked to rebase your changes. git checkout -b my_feature # rebase if necessary git fetch upstream main git rebase upstream/main Commits should be as small as possible, while ensuring that each commit is correct independently (i.e. each commit should compile and pass tests). Create your patch and test your changes before you commit them. Automated test by unit / integration tests are preferred. If tested manually, provide information about the test scope in the PR description. Now you can commit your changes to your feature branch and push it to your fork. git add . git commit -m \"Something meaningful\" git push origin my_feature Note Alternatively you can amend your commit before pushing if you forgot something by using git commit --amend Create Work In Progress [WIP] pull requests only if you need a clarification or an explicit review before you can continue your work item. If your patch is not getting reviewed, or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment. Post review: If a reviewer requires you to change your commit(s), please test the changes again. Amend the affected commit(s) and force push onto your branch. Set respective comments in your GitHub review as resolved. Create a general PR comment to notify the reviewers that your amendments are ready for another round of review. Issues and Planning \u00b6 We use GitHub issues to track bugs and enhancement requests. Please provide as much context as possible when you open an issue. The information you provide must be comprehensive enough to understand, reproduce the behavior and find related reports of that issue for the assignee. Therefore, contributors may use but aren't restricted to the issue template provided by the Gardener on Metal maintainers. Issues and pull requests are tracked in the backlog for this project.","title":"Contribution Guide"},{"location":"development/contribution/#contributors-guide","text":"","title":"Contributors Guide"},{"location":"development/contribution/#contributing","text":"The Gardener on Metal project uses Github to manage reviews of pull requests. If you are looking to make your first contribution, follow Steps to Contribute If you have a trivial fix or improvement, go ahead and create a pull request and address (with @...) a suitable maintainer of this repository (see CODEOWNERS of this repository) in the description of the pull request. If you plan to do something more involved, first discuss your ideas by creating an issue for this repository. This will avoid unnecessary work and surely give you and us a good deal of inspiration. Note Please follow these style guidelines to have your contribution considered by the maintainers: Coding style guidelines Go Code Review Comments , Formatting and style section of Peter Bourgon\u2019s Go: Best Practices for Production Environments .","title":"Contributing"},{"location":"development/contribution/#steps-to-contribute","text":"Do you want to work on an issue? You are welcome to claim an existing one by commenting on it in GitHub. Note Perform a cursory search to see if the issue has already been taken by someone else. This will prevent misunderstanding and duplication of effort from contributors on the same issue. If you have questions about one of the issues please comment on them and one of the maintainers will clarify it. We kindly ask you to follow the Pull Request Checklist to ensure reviews can happen accordingly.","title":"Steps to Contribute"},{"location":"development/contribution/#contributing-code","text":"You are welcome to contribute code to the Gardener on Metal project in order to fix a bug or to implement a new feature. The following rules govern code contributions: Contributions must be licensed under the Apache 2.0 License You need to sign the Developer Certificate of Origin .","title":"Contributing Code"},{"location":"development/contribution/#contributing-documentation","text":"You are welcome to contribute documentation to the Gardener on Metal project. The following rules govern documentation contributions: Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Developer Certificate of Origin .","title":"Contributing Documentation"},{"location":"development/contribution/#developer-certificate-of-origin","text":"Due to legal reasons, contributors will be asked to accept a Developer Certificate of Origin ( DCO ) before they submit the first pull request to the Gardener on Metal project, this happens in an automated fashion during the submission process. We use the standard DCO text of the Linux Foundation .","title":"Developer Certificate of Origin"},{"location":"development/contribution/#pull-request-checklist","text":"Fork and clone the repository to you local machine. git clone git@github.com:YOUR_GITHUB_USER/virtual-controlplane.git cd virtual-controlplane Create a branch from the main using 'git checkout' command. Note If needed, rebase to the current main branch before submitting your pull request. If it doesn't merge properly with main you may be asked to rebase your changes. git checkout -b my_feature # rebase if necessary git fetch upstream main git rebase upstream/main Commits should be as small as possible, while ensuring that each commit is correct independently (i.e. each commit should compile and pass tests). Create your patch and test your changes before you commit them. Automated test by unit / integration tests are preferred. If tested manually, provide information about the test scope in the PR description. Now you can commit your changes to your feature branch and push it to your fork. git add . git commit -m \"Something meaningful\" git push origin my_feature Note Alternatively you can amend your commit before pushing if you forgot something by using git commit --amend Create Work In Progress [WIP] pull requests only if you need a clarification or an explicit review before you can continue your work item. If your patch is not getting reviewed, or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment. Post review: If a reviewer requires you to change your commit(s), please test the changes again. Amend the affected commit(s) and force push onto your branch. Set respective comments in your GitHub review as resolved. Create a general PR comment to notify the reviewers that your amendments are ready for another round of review.","title":"Pull Request Checklist"},{"location":"development/contribution/#issues-and-planning","text":"We use GitHub issues to track bugs and enhancement requests. Please provide as much context as possible when you open an issue. The information you provide must be comprehensive enough to understand, reproduce the behavior and find related reports of that issue for the assignee. Therefore, contributors may use but aren't restricted to the issue template provided by the Gardener on Metal maintainers. Issues and pull requests are tracked in the backlog for this project.","title":"Issues and Planning"},{"location":"development/documentation/","text":"Documentation Setup \u00b6 The documentation of the virtual-controlplane project is written primarily using Markdown. All documentation related content can be found in the /docs folder. New content also should be added there. MkDocs and MkDocs Material are then used to render the contents of the /docs folder to have a more user-friendly experience when browsing the projects' documentation. Requirements: \u00b6 Following tools are required to work on that package. Kubernetes cluster access to deploy and test the result (via minikube, kind or docker desktop locally) make - to execute build goals docker - to run the local mkdocs environment git - to be able to commit any changes to repository Note If you don't have Docker installed on your machine please follow one of those guides: Docker Desktop for Mac Docker Desktop for Windows Docker Engine for Linux Local Development Setup \u00b6 This project contains a local Docker based runtime environment for the documentation part. If you have an access to the docker registry and k8s installation that you can use for development purposes, just run following command and access the output in your browser under http://localhost:8000/ : make start-docs The environment will hot-rebuild your documentation, so there is no need to restart it while you make your changes. If you want to add a new chapter (basically a new file/folder to docs directory) you should add it to the nav section in the mkdocs.yml file in the projects root folder. Use helper Makefile directive to clean up old and stopped container instances. make clean-docs Writing Content \u00b6 Abbreviations \u00b6 Abbreviations are defined centrally in the following file /hack/docs/abbreviations.md . In case you introduce any new abbreviation to your content, please make sure to add a corresponding entry there. Please include the statement --8<-- \"hack/docs/abbreviations.md\" at the end of each Markdown file. This will ensure that the abbreviation highlighting will work correctly.","title":"Documentation"},{"location":"development/documentation/#documentation-setup","text":"The documentation of the virtual-controlplane project is written primarily using Markdown. All documentation related content can be found in the /docs folder. New content also should be added there. MkDocs and MkDocs Material are then used to render the contents of the /docs folder to have a more user-friendly experience when browsing the projects' documentation.","title":"Documentation Setup"},{"location":"development/documentation/#requirements","text":"Following tools are required to work on that package. Kubernetes cluster access to deploy and test the result (via minikube, kind or docker desktop locally) make - to execute build goals docker - to run the local mkdocs environment git - to be able to commit any changes to repository Note If you don't have Docker installed on your machine please follow one of those guides: Docker Desktop for Mac Docker Desktop for Windows Docker Engine for Linux","title":"Requirements:"},{"location":"development/documentation/#local-development-setup","text":"This project contains a local Docker based runtime environment for the documentation part. If you have an access to the docker registry and k8s installation that you can use for development purposes, just run following command and access the output in your browser under http://localhost:8000/ : make start-docs The environment will hot-rebuild your documentation, so there is no need to restart it while you make your changes. If you want to add a new chapter (basically a new file/folder to docs directory) you should add it to the nav section in the mkdocs.yml file in the projects root folder. Use helper Makefile directive to clean up old and stopped container instances. make clean-docs","title":"Local Development Setup"},{"location":"development/documentation/#writing-content","text":"","title":"Writing Content"},{"location":"development/documentation/#abbreviations","text":"Abbreviations are defined centrally in the following file /hack/docs/abbreviations.md . In case you introduce any new abbreviation to your content, please make sure to add a corresponding entry there. Please include the statement --8<-- \"hack/docs/abbreviations.md\" at the end of each Markdown file. This will ensure that the abbreviation highlighting will work correctly.","title":"Abbreviations"},{"location":"usage/access/","text":"Access the Virtual Control Plane API Server \u00b6 The template-operator is generating a secret containing the initial kubeconfig with admin privileges. It is located in the namespace of your virtual-controlplane installation. Get initial admin kubeconfig \u00b6 The kubeconfig can be read from the cluster by running the following command: kubectl -n onmetal get secret kubeconfig-for-admin \\ -o jsonpath ={ .data.kubeconfig } | base64 -d > kubeconfig Note base64 cli is needed to decrypt the base64 encoded secret values. Here we assume, that onmetal is our namespace where we installed the virtual control plane components into. Accessing the cluster \u00b6 The last thing we need to do is to set the KUBECONFIG environment variable to point to our newly created kubeconfig file: export KUBECONFIG = kubeconfig If everything worked out, we should now be able to access the cluster via: kubectl get namespaces","title":"Access API Server"},{"location":"usage/access/#access-the-virtual-control-plane-api-server","text":"The template-operator is generating a secret containing the initial kubeconfig with admin privileges. It is located in the namespace of your virtual-controlplane installation.","title":"Access the Virtual Control Plane API Server"},{"location":"usage/access/#get-initial-admin-kubeconfig","text":"The kubeconfig can be read from the cluster by running the following command: kubectl -n onmetal get secret kubeconfig-for-admin \\ -o jsonpath ={ .data.kubeconfig } | base64 -d > kubeconfig Note base64 cli is needed to decrypt the base64 encoded secret values. Here we assume, that onmetal is our namespace where we installed the virtual control plane components into.","title":"Get initial admin kubeconfig"},{"location":"usage/access/#accessing-the-cluster","text":"The last thing we need to do is to set the KUBECONFIG environment variable to point to our newly created kubeconfig file: export KUBECONFIG = kubeconfig If everything worked out, we should now be able to access the cluster via: kubectl get namespaces","title":"Accessing the cluster"},{"location":"usage/cert-manager/","text":"CertManager \u00b6 We are using the cert-manager to generate and update TLS certificates. Please refer to the cert-manager installation documentation for details. Installation \u00b6 Add the cert-manager helm repository: helm repo add jetstack https://charts.jetstack.io helm repo update The cert-manager can be installed via: helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.5.3 \\ --set installCRDs = true \\ --set extraArgs ={ \"--enable-certificate-owner-ref=true\" } Upgrade \u00b6 helm -n cert-manager upgrade cert-manager \\ jetstack/cert-manager -f VALUES_DIR/cert-manager-values.yaml Remove \u00b6 To uninstall the cert-manager from your cluster run: helm -n cert-manager delete cert-manager","title":"Cert Manager"},{"location":"usage/cert-manager/#certmanager","text":"We are using the cert-manager to generate and update TLS certificates. Please refer to the cert-manager installation documentation for details.","title":"CertManager"},{"location":"usage/cert-manager/#installation","text":"Add the cert-manager helm repository: helm repo add jetstack https://charts.jetstack.io helm repo update The cert-manager can be installed via: helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.5.3 \\ --set installCRDs = true \\ --set extraArgs ={ \"--enable-certificate-owner-ref=true\" }","title":"Installation"},{"location":"usage/cert-manager/#upgrade","text":"helm -n cert-manager upgrade cert-manager \\ jetstack/cert-manager -f VALUES_DIR/cert-manager-values.yaml","title":"Upgrade"},{"location":"usage/cert-manager/#remove","text":"To uninstall the cert-manager from your cluster run: helm -n cert-manager delete cert-manager","title":"Remove"},{"location":"usage/druid/","text":"Etcd Druid \u00b6 The etcd-druid is an operator which manages etcd deployments inside a Kubernetes cluster. Installation \u00b6 etcd-druid can be installed via helm install into your Kubernetes cluster. Please make sure you have configured the helm repo as described here . helm -n onmetal install druid vc/etcd-druid --create-namespace If you want to overwrite the default values.yaml you can do so by providing a custom configuration. helm -n onmetal install druid vc/etcd-druid -f VALUES_DIR/druid-values.yaml Note Please refer to the chart readme for detailed information on the value parameters. Upgrade \u00b6 To upgrade your etcd-druid deployment run: helm -n onmetal upgrade druid vc/etcd-druid -f VALUES_DIR/druid-values.yaml Remove \u00b6 To remove the installation run: helm -n onmetal delete druid","title":"Druid"},{"location":"usage/druid/#etcd-druid","text":"The etcd-druid is an operator which manages etcd deployments inside a Kubernetes cluster.","title":"Etcd Druid"},{"location":"usage/druid/#installation","text":"etcd-druid can be installed via helm install into your Kubernetes cluster. Please make sure you have configured the helm repo as described here . helm -n onmetal install druid vc/etcd-druid --create-namespace If you want to overwrite the default values.yaml you can do so by providing a custom configuration. helm -n onmetal install druid vc/etcd-druid -f VALUES_DIR/druid-values.yaml Note Please refer to the chart readme for detailed information on the value parameters.","title":"Installation"},{"location":"usage/druid/#upgrade","text":"To upgrade your etcd-druid deployment run: helm -n onmetal upgrade druid vc/etcd-druid -f VALUES_DIR/druid-values.yaml","title":"Upgrade"},{"location":"usage/druid/#remove","text":"To remove the installation run: helm -n onmetal delete druid","title":"Remove"},{"location":"usage/etcd/","text":"Etcd \u00b6 etcd is the main backend storage of the Kubernetes API server. We will be using our etcd-druid operator to deploy a production ready etcd cluster. Installation \u00b6 etcd can be installed via helm install into your Kubernetes cluster. Please make sure you have configured the helm repo as described here . helm -n onmetal install etcd vc/etcd If you want to overwrite the default values.yaml you can do so by providing a custom configuration. helm -n onmetal install etcd vc/etcd -f VALUES_DIR/etcd-values.yaml Note Please refer to the chart README for detailed information on the value parameters. Upgrade \u00b6 To upgrade your etcd deployment run: helm -n onmetal upgrade etcd vc/etcd -f VALUES_DIR/etcd-values.yaml Remove \u00b6 To remove the installation run: helm -n onmetal delete etcd","title":"Etcd"},{"location":"usage/etcd/#etcd","text":"etcd is the main backend storage of the Kubernetes API server. We will be using our etcd-druid operator to deploy a production ready etcd cluster.","title":"Etcd"},{"location":"usage/etcd/#installation","text":"etcd can be installed via helm install into your Kubernetes cluster. Please make sure you have configured the helm repo as described here . helm -n onmetal install etcd vc/etcd If you want to overwrite the default values.yaml you can do so by providing a custom configuration. helm -n onmetal install etcd vc/etcd -f VALUES_DIR/etcd-values.yaml Note Please refer to the chart README for detailed information on the value parameters.","title":"Installation"},{"location":"usage/etcd/#upgrade","text":"To upgrade your etcd deployment run: helm -n onmetal upgrade etcd vc/etcd -f VALUES_DIR/etcd-values.yaml","title":"Upgrade"},{"location":"usage/etcd/#remove","text":"To remove the installation run: helm -n onmetal delete etcd","title":"Remove"},{"location":"usage/helm/","text":"Helm Repository \u00b6 The virtual-controlplane project comes with it's own Helm repository containing all Helm charts necessary to setup a virtual Kubernetes API server. It can be added to your Helm environment via: helm repo add vc https://onmetal.github.io/virtual-controlplane/ To update the Helm repository: helm repo update You should now be able to see the content of the helm repository via: helm search repo vc An example output would look like this: NAME CHART VERSION APP VERSION DESCRIPTION vc/etcd 0 .1.1 3 .4.13 Helm chart for deploying etcd through etcd druid. vc/etcd-druid 0 .1.2 0 .6.0 Helm chart for etcd-druid. vc/virtual-controlplane 0 .1.2 1 .22.1 A Helm chart for a Virtual Kubernetes controlplane","title":"Helm Repository"},{"location":"usage/helm/#helm-repository","text":"The virtual-controlplane project comes with it's own Helm repository containing all Helm charts necessary to setup a virtual Kubernetes API server. It can be added to your Helm environment via: helm repo add vc https://onmetal.github.io/virtual-controlplane/ To update the Helm repository: helm repo update You should now be able to see the content of the helm repository via: helm search repo vc An example output would look like this: NAME CHART VERSION APP VERSION DESCRIPTION vc/etcd 0 .1.1 3 .4.13 Helm chart for deploying etcd through etcd druid. vc/etcd-druid 0 .1.2 0 .6.0 Helm chart for etcd-druid. vc/virtual-controlplane 0 .1.2 1 .22.1 A Helm chart for a Virtual Kubernetes controlplane","title":"Helm Repository"},{"location":"usage/installation/","text":"Installation \u00b6 This section describes all installation steps for virtual-controlplane into a Kubernetes cluster. Prerequisites \u00b6 helm kubectl Access to a Kubernetes cluster ( minikube , kind or a real one) Components \u00b6 Required components to deploy the virtual control plane are depicted in the diagram below: The utility controller in this setup is needed for the following tasks: cert-manager : Manages cluster wide CAs and certificates for the kube-apiserver and etcd template-operator : Generates in-cluster kubeconfigs from the genereated TLS certificates druid : Deploys and manages the lifecycle of etcd clusters Installation \u00b6 To install all the components mentioned above please follow the guides in the following order: Configure Helm repository Install Cert-Manager Install Druid Install Etcd Install Template Operator Install Virtual Control Plane Components Access API Server \u00b6 Please follow the Access API Server to access the virtual Kubernetes API server we just deployed.","title":"Installation"},{"location":"usage/installation/#installation","text":"This section describes all installation steps for virtual-controlplane into a Kubernetes cluster.","title":"Installation"},{"location":"usage/installation/#prerequisites","text":"helm kubectl Access to a Kubernetes cluster ( minikube , kind or a real one)","title":"Prerequisites"},{"location":"usage/installation/#components","text":"Required components to deploy the virtual control plane are depicted in the diagram below: The utility controller in this setup is needed for the following tasks: cert-manager : Manages cluster wide CAs and certificates for the kube-apiserver and etcd template-operator : Generates in-cluster kubeconfigs from the genereated TLS certificates druid : Deploys and manages the lifecycle of etcd clusters","title":"Components"},{"location":"usage/installation/#installation_1","text":"To install all the components mentioned above please follow the guides in the following order: Configure Helm repository Install Cert-Manager Install Druid Install Etcd Install Template Operator Install Virtual Control Plane Components","title":"Installation"},{"location":"usage/installation/#access-api-server","text":"Please follow the Access API Server to access the virtual Kubernetes API server we just deployed.","title":"Access API Server"},{"location":"usage/template/","text":"Template Operator \u00b6 The template-operator is used to create derived objects from other Kubernetes objects. It is for example possible to create a Secret containing the kubeconfig from other Secrets or ConfigMaps which contains the TLS certificates and other information needed to generate a kubeconfig . Installation \u00b6 Add the template-operator helm chart repository: helm repo add template-operator https://onmetal.github.io/template-operator/ helm repo update The template-operator helm chart can be installed via: helm install \\ template-operator template-operator/template-operator \\ --namespace template-operator \\ --create-namespace Configuration \u00b6 The template-operator chart can be configured by overriding the default values . The RBAC rules are important configuration point, as we need to specify which objects the template-operator is allowed to watch and modify. Here is an example, as defined in the default values.yaml , to allow to watch and modify Secrets and ConfigMaps in your cluster. rbac : adminGroupResouces : - groups : - \"\" resources : - secrets - configmaps Upgrade \u00b6 To upgrade your template-operator deployment run: helm -n template-operator upgrade template-operator \\ template-operator/template-operator \\ -f VALUES_DIR/template-operator-values.yaml Remove \u00b6 To uninstall the template-operator from your cluster run: helm -n template-operator delete template-operator","title":"Template Operator"},{"location":"usage/template/#template-operator","text":"The template-operator is used to create derived objects from other Kubernetes objects. It is for example possible to create a Secret containing the kubeconfig from other Secrets or ConfigMaps which contains the TLS certificates and other information needed to generate a kubeconfig .","title":"Template Operator"},{"location":"usage/template/#installation","text":"Add the template-operator helm chart repository: helm repo add template-operator https://onmetal.github.io/template-operator/ helm repo update The template-operator helm chart can be installed via: helm install \\ template-operator template-operator/template-operator \\ --namespace template-operator \\ --create-namespace","title":"Installation"},{"location":"usage/template/#configuration","text":"The template-operator chart can be configured by overriding the default values . The RBAC rules are important configuration point, as we need to specify which objects the template-operator is allowed to watch and modify. Here is an example, as defined in the default values.yaml , to allow to watch and modify Secrets and ConfigMaps in your cluster. rbac : adminGroupResouces : - groups : - \"\" resources : - secrets - configmaps","title":"Configuration"},{"location":"usage/template/#upgrade","text":"To upgrade your template-operator deployment run: helm -n template-operator upgrade template-operator \\ template-operator/template-operator \\ -f VALUES_DIR/template-operator-values.yaml","title":"Upgrade"},{"location":"usage/template/#remove","text":"To uninstall the template-operator from your cluster run: helm -n template-operator delete template-operator","title":"Remove"},{"location":"usage/vc/","text":"Virtual Control Plane \u00b6 The virtual-controlplane helm chart is installing the kube-apiserver and kube-controller-manager which are the core building blocks of our virtual Kubernetes cluster. Prerequisites \u00b6 Nginx ingress controller is installed in your Kubernetes cluster DNS is configured for your external Ingress load balancer Configuration \u00b6 Before we begin with the installation it is recommended to override some of the default values . To do that we need to create a values.yaml file containing the following entries: apiServer : externalDNSName : <external ingress DNS name> tls : staticToken : securePasswordToken Note The helm installation will run through without changing those values. This might be OK if you deploy it into a local Minikube or Kind cluster. However you won't be able to access your cluster since the Ingress won't be configured correctly. Note, that there is a huge security risk as the static token can be guessed relatively easy exposing your API server to fraudulent access. Installation \u00b6 The virtual-controlplane chart can be installed via: helm install \\ vc vc/virtual-controlplane \\ --namespace onmetal \\ --create-namespace \\ -f VALUES_DIR/values.yaml Access API Server \u00b6 Please follow the Access API Server to access the virtual Kubernetes API server we just deployed. Upgrade \u00b6 To upgrade your virtual-controlplane deployment run: helm -n onmetal upgrade vc vc/virtual-controlplane -f VALUES_DIR/values.yaml Remove \u00b6 To remove the installation run: helm -n onmetal delete vc","title":"Virtual Control Plane"},{"location":"usage/vc/#virtual-control-plane","text":"The virtual-controlplane helm chart is installing the kube-apiserver and kube-controller-manager which are the core building blocks of our virtual Kubernetes cluster.","title":"Virtual Control Plane"},{"location":"usage/vc/#prerequisites","text":"Nginx ingress controller is installed in your Kubernetes cluster DNS is configured for your external Ingress load balancer","title":"Prerequisites"},{"location":"usage/vc/#configuration","text":"Before we begin with the installation it is recommended to override some of the default values . To do that we need to create a values.yaml file containing the following entries: apiServer : externalDNSName : <external ingress DNS name> tls : staticToken : securePasswordToken Note The helm installation will run through without changing those values. This might be OK if you deploy it into a local Minikube or Kind cluster. However you won't be able to access your cluster since the Ingress won't be configured correctly. Note, that there is a huge security risk as the static token can be guessed relatively easy exposing your API server to fraudulent access.","title":"Configuration"},{"location":"usage/vc/#installation","text":"The virtual-controlplane chart can be installed via: helm install \\ vc vc/virtual-controlplane \\ --namespace onmetal \\ --create-namespace \\ -f VALUES_DIR/values.yaml","title":"Installation"},{"location":"usage/vc/#access-api-server","text":"Please follow the Access API Server to access the virtual Kubernetes API server we just deployed.","title":"Access API Server"},{"location":"usage/vc/#upgrade","text":"To upgrade your virtual-controlplane deployment run: helm -n onmetal upgrade vc vc/virtual-controlplane -f VALUES_DIR/values.yaml","title":"Upgrade"},{"location":"usage/vc/#remove","text":"To remove the installation run: helm -n onmetal delete vc","title":"Remove"}]}