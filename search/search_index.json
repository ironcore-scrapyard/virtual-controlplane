{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Virtual Controlplane \u00b6 The project contains a collection of helm charts to deploy a virtual/nodeless Kubernetes API server . Background \u00b6 Why do we need such a thing as a nodeless Kubernetes cluster? When developing a custom controlplane by extending Kubernetes and using the Operator Pattern we typically might not be interested in objects like Pods , Deployments and others. The goal instead is to introduce new CustomResourceDefinitions on which our controllers will operate by leveraging the concepts the Kubernetes API machinery is offering us. In order to do it, we will need certain things such as: etcd server (ideally with a backup and restore sidecar 1 ) kube-api-server and kube-controller-manager Since this Kubernetes API server setup does not have any nodes, we don't need to deploy the kube-scheduler as we won't be deploying any Pods . Those components can be easily deployed on an existing Kubernetes cluster 2 . The figure above illustrates the core components which are deployed on a vanilla Kubernetes runtime cluster. Your custom controllers can now also be deployed on the same (or different) runtime cluster as illustrated in the image above. Advantages \u00b6 The main advantage of this approach is, that controllers and the content inside of virtual Kubernetes API server are no longer physically bound to the underlying runtime cluster as your CRDs are not mixed with content of the runtime clusters API server. Since the virtual controlplane in our scenario is just another workload running in a Kubernetes cluster, it can be moved and restored to a different cluster in case disaster strikes and the runtime cluster becomes unavailable. Another advantage is, that the performance of the virtual Kubernetes API server becomes a lot more predictable since nobody besides your controllers 3 are working against it. In our setup we are reusing the great work from the Gardener project. \u21a9 This can also be done on a classical virtual machine based setup. \u21a9 And maybe the kube-controller-manager . \u21a9","title":"Home"},{"location":"#virtual-controlplane","text":"The project contains a collection of helm charts to deploy a virtual/nodeless Kubernetes API server .","title":"Virtual Controlplane"},{"location":"#background","text":"Why do we need such a thing as a nodeless Kubernetes cluster? When developing a custom controlplane by extending Kubernetes and using the Operator Pattern we typically might not be interested in objects like Pods , Deployments and others. The goal instead is to introduce new CustomResourceDefinitions on which our controllers will operate by leveraging the concepts the Kubernetes API machinery is offering us. In order to do it, we will need certain things such as: etcd server (ideally with a backup and restore sidecar 1 ) kube-api-server and kube-controller-manager Since this Kubernetes API server setup does not have any nodes, we don't need to deploy the kube-scheduler as we won't be deploying any Pods . Those components can be easily deployed on an existing Kubernetes cluster 2 . The figure above illustrates the core components which are deployed on a vanilla Kubernetes runtime cluster. Your custom controllers can now also be deployed on the same (or different) runtime cluster as illustrated in the image above.","title":"Background"},{"location":"#advantages","text":"The main advantage of this approach is, that controllers and the content inside of virtual Kubernetes API server are no longer physically bound to the underlying runtime cluster as your CRDs are not mixed with content of the runtime clusters API server. Since the virtual controlplane in our scenario is just another workload running in a Kubernetes cluster, it can be moved and restored to a different cluster in case disaster strikes and the runtime cluster becomes unavailable. Another advantage is, that the performance of the virtual Kubernetes API server becomes a lot more predictable since nobody besides your controllers 3 are working against it. In our setup we are reusing the great work from the Gardener project. \u21a9 This can also be done on a classical virtual machine based setup. \u21a9 And maybe the kube-controller-manager . \u21a9","title":"Advantages"},{"location":"development/contribution/","text":"","title":"Contribution Guide"},{"location":"development/documentation/","text":"","title":"Documentation"},{"location":"development/setup/","text":"","title":"Setup"},{"location":"development/testing/","text":"","title":"Testing"},{"location":"usage/local_setup/","text":"Local Setup \u00b6","title":"Local Setup"},{"location":"usage/local_setup/#local-setup","text":"","title":"Local Setup"}]}